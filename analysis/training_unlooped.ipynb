{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import aim\n",
    "import glob\n",
    "import random\n",
    "\n",
    "os.environ[\"CONFIG_PATHS\"] = \"../configs/training_unlooped.yaml\"\n",
    "os.environ[\"CONFIG_OVERRIDES\"] = 'game.moves_directory=\"../data/moves_10\"'\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import time\n",
    "from configuration import moves_data, config\n",
    "from training.load_games import load_games_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config:  {\"game\": {\"board_size\": 10, \"num_moves\": 6233, \"num_pieces\": 21, \"moves_directory\": \"../data/moves_10\"}, \"training\": {\"batch_size\": 128, \"policy_loss_weight\": 0.158, \"learning_rate\": 0.001}, \"networks\": {\"default\": {\"main_body_channels\": 64, \"value_head_channels\": 16, \"value_head_flat_layer_width\": 64, \"policy_head_channels\": 32, \"unused_pieces_flat_layer_width\": 32, \"residual_blocks\": 10}}, \"agents\": []}\n"
     ]
    }
   ],
   "source": [
    "from neural_net import NeuralNet\n",
    "from training.game_data_manager import GameDataManager, DirectoryGameDataPathFetcher, CustomGameDataPathFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_CONFIG = config()[\"networks\"][\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: piece_indices\n",
      "Loading file: rotation_mapping\n",
      "Loading file: new_occupieds\n",
      "Loading file: moves_ruled_out_for_all\n",
      "Loading file: scores\n",
      "Loading file: moves_ruled_out_for_player\n",
      "Loading file: moves_enabled_for_player\n",
      "Loading file: new_adjacents\n",
      "Loading file: new_corners\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "DEVICE = \"mps\"\n",
    "MOVES = moves_data()\n",
    "GAMES_DIR = \"../data/2024-11-23_00-37-50-doublehandedness/games\"\n",
    "POLICY_LOSS_WEIGHT = 0.158\n",
    "LEARNING_RATE = 1e-2  # previously 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNet(NETWORK_CONFIG)\n",
    "model.to(DEVICE)\n",
    "\n",
    "model.load_state_dict(torch.load(\"/Users/shivamsarodia/Dev/blockus/data/notebook-models/53766cc2437f462ebf478dab/epoch_4.pt\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_test_losses(model, test_dataset, exclude_invalid_moves=False):\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    results = {\n",
    "        \"total_loss\": 0,\n",
    "        \"value_loss\": 0,\n",
    "        \"policy_loss\": 0,\n",
    "        \"value_max_correct\": 0,\n",
    "        \"policy_max_correct\": 0,\n",
    "    }\n",
    "\n",
    "    total_sample_count = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for boards, policies, values, unused_pieces, valid_moves in dataloader:\n",
    "            boards = boards.to(dtype=torch.float32, device=DEVICE)\n",
    "            policies = policies.to(dtype=torch.float32, device=DEVICE)\n",
    "            values = values.to(dtype=torch.float32, device=DEVICE)\n",
    "            unused_pieces = unused_pieces.to(dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            pred_values, pred_policy_logits = model(boards, unused_pieces)\n",
    "\n",
    "            if exclude_invalid_moves:\n",
    "                pred_policy_logits[~valid_moves] = -1e9\n",
    "\n",
    "            value_loss = nn.CrossEntropyLoss(reduction=\"sum\")(\n",
    "                pred_values,\n",
    "                values,\n",
    "            )\n",
    "            policy_loss = nn.CrossEntropyLoss(reduction=\"sum\")(\n",
    "                pred_policy_logits,\n",
    "                policies,\n",
    "            )\n",
    "            loss = value_loss + POLICY_LOSS_WEIGHT * policy_loss\n",
    "\n",
    "            results[\"total_loss\"] += loss.item()\n",
    "            results[\"value_loss\"] += value_loss.item()\n",
    "            results[\"policy_loss\"] += policy_loss.item()\n",
    "            results[\"value_max_correct\"] += (pred_values.argmax(dim=1) == values.argmax(dim=1)).sum().item()\n",
    "            results[\"policy_max_correct\"] += (pred_policy_logits.argmax(dim=1) == policies.argmax(dim=1)).sum().item()\n",
    "\n",
    "            total_sample_count += len(boards)\n",
    "\n",
    "    results[\"total_loss\"] /= total_sample_count\n",
    "    results[\"value_loss\"] /= total_sample_count\n",
    "    results[\"policy_loss\"] /= total_sample_count\n",
    "    results[\"value_max_correct\"] /= total_sample_count\n",
    "    results[\"policy_max_correct\"] /= total_sample_count\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:03<00:00, 52.53it/s]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_TEST_SPLIT = 0.9\n",
    "\n",
    "def get_dataset(game_files):\n",
    "    gamedata = load_games_new(game_files, with_tqdm=True)\n",
    "    boards_tensor = torch.from_numpy(gamedata[\"boards\"]).to(dtype=torch.float)\n",
    "    policies_tensor = torch.from_numpy(gamedata[\"policies\"]).to(dtype=torch.float)\n",
    "    values_tensor = torch.from_numpy(gamedata[\"values\"]).to(dtype=torch.float)\n",
    "    unused_pieces = torch.from_numpy(gamedata[\"unused_pieces\"]).to(dtype=torch.bool)\n",
    "    valid_moves = torch.from_numpy(gamedata[\"valid_moves\"]).to(dtype=torch.bool)\n",
    "    return torch.utils.data.TensorDataset(boards_tensor, policies_tensor, values_tensor, unused_pieces, valid_moves)\n",
    "\n",
    "file_paths = glob.glob(\"/Users/shivamsarodia/Dev/blockus/data/2024-11-23_00-37-50-doublehandedness/untrained_games_*/*.npz\")\n",
    "\n",
    "random.seed(20554)\n",
    "random.shuffle(file_paths)\n",
    "\n",
    "num_train_games = int(len(file_paths) * TRAIN_TEST_SPLIT)\n",
    "train_file_paths = file_paths[:num_train_games]\n",
    "test_file_paths = file_paths[num_train_games:]\n",
    "\n",
    "test_dataset = get_dataset(test_file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = get_test_losses(model, test_dataset)\n",
    "test_losses_exclude_invalid_moves = get_test_losses(model, test_dataset, exclude_invalid_moves=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_loss': 1.140363884838364,\n",
       " 'value_loss': 0.9053822001797702,\n",
       " 'policy_loss': 1.487225781700051,\n",
       " 'value_max_correct': 0.6013491489091342,\n",
       " 'policy_max_correct': 0.856560388501848}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_loss': 1.1387235602985408,\n",
       " 'value_loss': 0.9053822002614371,\n",
       " 'policy_loss': 1.4768439857763338,\n",
       " 'value_max_correct': 0.6013491489091342,\n",
       " 'policy_max_correct': 0.8586377021375337}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_losses_exclude_invalid_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    model_path = os.path.join(\n",
    "        \"/Users/shivamsarodia/Dev/blockus/data/2024-12-02_21-22-57-notebook-models/sample_ratio_two_partials/\",\n",
    "        f\"{model_name}.pt\",\n",
    "    )\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_game_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     valid_moves \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(gamedata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_moves\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTensorDataset(boards_tensor, policies_tensor, values_tensor, valid_moves)\n\u001b[0;32m---> 25\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m get_dataset(\u001b[43mtrain_game_files\u001b[49m)\n\u001b[1;32m     26\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m get_dataset(test_game_files)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_game_files' is not defined"
     ]
    }
   ],
   "source": [
    "# games_files = sorted(glob.glob(\"/Users/shivamsarodia/Dev/blockus/data/2024-11-23_00-37-50-doublehandedness/games/*.npz\"))\n",
    "# games_we_didnt_train_on = sorted(glob.glob(\"/Users/shivamsarodia/Dev/blockus/data/2024-11-23_00-37-50-doublehandedness/games_we_didnt_train_on/*.npz\"))\n",
    "# more_games_we_didnt_train_on = sorted(glob.glob(\"/Users/shivamsarodia/Dev/blockus/data/2024-11-23_00-37-50-doublehandedness/more_games_we_didnt_train_on/*.npz\"))\n",
    "\n",
    "# recent_games_files = [f for f in games_files if f > \"/Users/shivamsarodia/Dev/blockus/data/2024-11-23_00-37-50-doublehandedness/games/1732975250110_851.npz\"]\n",
    "# all_games = recent_games_files + games_we_didnt_train_on + more_games_we_didnt_train_on\n",
    "\n",
    "# random.seed(20554)\n",
    "# random.shuffle(all_games)\n",
    "\n",
    "# train_game_files = all_games[200:]\n",
    "# test_game_files = all_games[:200]\n",
    "\n",
    "# print(\"Num train files:\", len(train_game_files))\n",
    "# print(\"Num test files:\", len(test_game_files))\n",
    "\n",
    "def get_dataset(game_files):\n",
    "    gamedata = load_games_new(game_files, with_tqdm=True)\n",
    "    boards_tensor = torch.from_numpy(gamedata[\"boards\"]).to(dtype=torch.float)\n",
    "    policies_tensor = torch.from_numpy(gamedata[\"policies\"]).to(dtype=torch.float)\n",
    "    values_tensor = torch.from_numpy(gamedata[\"values\"]).to(dtype=torch.float)\n",
    "    valid_moves = torch.from_numpy(gamedata[\"valid_moves\"]).to(dtype=torch.bool)\n",
    "    return torch.utils.data.TensorDataset(boards_tensor, policies_tensor, values_tensor, valid_moves)\n",
    "\n",
    "train_dataset = get_dataset(train_game_files)\n",
    "test_dataset = get_dataset(test_game_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train samples: 983233\n",
      "Num test samples: 172325\n"
     ]
    }
   ],
   "source": [
    "print(\"Num train samples:\", len(train_dataset))\n",
    "print(\"Num test samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_losses(model):\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    results = {\n",
    "        \"total_loss\": 0,\n",
    "        \"value_loss\": 0,\n",
    "        \"policy_loss\": 0,\n",
    "        \"value_max_correct\": 0,\n",
    "        \"policy_max_correct\": 0,\n",
    "    }\n",
    "\n",
    "    total_sample_count = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for boards, policies, values, valid_moves in dataloader:\n",
    "            boards = boards.to(dtype=torch.float32, device=DEVICE)\n",
    "            policies = policies.to(dtype=torch.float32, device=DEVICE)\n",
    "            values = values.to(dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            pred_values, pred_policy_logits = model(boards.to(dtype=torch.float32, device=DEVICE))\n",
    "            value_loss = nn.CrossEntropyLoss(reduction=\"sum\")(\n",
    "                pred_values,\n",
    "                values,\n",
    "            )\n",
    "            policy_loss = nn.CrossEntropyLoss(reduction=\"sum\")(\n",
    "                pred_policy_logits,\n",
    "                policies,\n",
    "            )\n",
    "            loss = value_loss + POLICY_LOSS_WEIGHT * policy_loss\n",
    "\n",
    "            results[\"total_loss\"] += loss.item()\n",
    "            results[\"value_loss\"] += value_loss.item()\n",
    "            results[\"policy_loss\"] += policy_loss.item()\n",
    "            results[\"value_max_correct\"] += (pred_values.argmax(dim=1) == values.argmax(dim=1)).sum().item()\n",
    "            results[\"policy_max_correct\"] += (pred_policy_logits.argmax(dim=1) == policies.argmax(dim=1)).sum().item()\n",
    "\n",
    "            total_sample_count += len(boards)\n",
    "\n",
    "    results[\"total_loss\"] /= total_sample_count\n",
    "    results[\"value_loss\"] /= total_sample_count\n",
    "    results[\"policy_loss\"] /= total_sample_count\n",
    "    results[\"value_max_correct\"] /= total_sample_count\n",
    "    results[\"policy_max_correct\"] /= total_sample_count\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7682/7682 [10:46<00:00, 11.89it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7682/7682 [10:50<00:00, 11.82it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7682/7682 [09:44<00:00, 13.15it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7682/7682 [12:04<00:00, 10.60it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7682/7682 [12:52<00:00,  9.94it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "run = aim.Run(repo='/Users/shivamsarodia/Dev/blockus/')\n",
    "run[\"hparams\"] = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"policy_loss_weight\": POLICY_LOSS_WEIGHT,\n",
    "}\n",
    "\n",
    "batch_index = 0\n",
    "for epoch in range(5):\n",
    "    for boards, policies, values, valid_moves in tqdm(train_dataloader):\n",
    "        model.train()\n",
    "\n",
    "        boards = boards.to(dtype=torch.float32, device=DEVICE)\n",
    "        policies = policies.to(dtype=torch.float32, device=DEVICE)\n",
    "        values = values.to(dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        pred_values, pred_policy = model(boards)\n",
    "        value_loss = nn.CrossEntropyLoss()(\n",
    "            pred_values,\n",
    "            values,\n",
    "        )\n",
    "        policy_loss = nn.CrossEntropyLoss()(\n",
    "            pred_policy,\n",
    "            policies,\n",
    "        )\n",
    "        loss = value_loss + POLICY_LOSS_WEIGHT * policy_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        training_result = {\n",
    "            \"total_loss\": loss.item(),\n",
    "            \"value_loss\": value_loss.item(),\n",
    "            \"policy_loss\": policy_loss.item(),\n",
    "            \"value_max_correct\": (pred_values.argmax(dim=1) == values.argmax(dim=1)).sum().item() / len(boards),\n",
    "            \"policy_max_correct\": (pred_policy.argmax(dim=1) == policies.argmax(dim=1)).sum().item() / len(boards),\n",
    "        }\n",
    "\n",
    "        for key, value in training_result.items():\n",
    "            run.track(\n",
    "                value,\n",
    "                name=key,\n",
    "                step=batch_index,\n",
    "                context={\"subset\": \"train\"},\n",
    "            )\n",
    "\n",
    "        if batch_index % 1000 == 0:\n",
    "            test_losses = get_test_losses(model)\n",
    "            for key, value in test_losses.items():\n",
    "                run.track(\n",
    "                    value,\n",
    "                    name=key,\n",
    "                    step=batch_index,\n",
    "                    context={\"subset\": \"test\"},\n",
    "                )\n",
    "        batch_index += 1\n",
    "\n",
    "    print(\"Finished epoch\")\n",
    "\n",
    "run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"../data/2024-12-02_21-22-57-notebook-models/sample_ratio_two_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172325/172325 [00:08<00:00, 19911.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique boards: 107526\n",
      "Number of boards with at least one piece: 164399\n",
      "Number of boards with at least two pieces: 156661\n",
      "Number of boards with at least three pieces: 148777\n",
      "Number of boards with at least four pieces: 140872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "boards_seen = set()\n",
    "total_boards_count = 0\n",
    "boards_with_at_least_one_piece_count = 0\n",
    "boards_with_at_least_two_pieces_count = 0\n",
    "boards_with_at_least_three_pieces_count = 0\n",
    "boards_with_at_least_four_pieces_count = 0\n",
    "for boards, policies, values, valid_moves in tqdm(count_dataloader):\n",
    "    boards_seen.add(boards.numpy(force=True).tobytes())\n",
    "    total_boards_count += 1\n",
    "    if torch.sum(boards) > 0:\n",
    "        boards_with_at_least_one_piece_count += 1\n",
    "    if torch.sum(boards) > 5:\n",
    "        boards_with_at_least_two_pieces_count += 1\n",
    "    if torch.sum(boards) > 10:\n",
    "        boards_with_at_least_three_pieces_count += 1\n",
    "    if torch.sum(boards) > 15:\n",
    "        boards_with_at_least_four_pieces_count += 1\n",
    "        \n",
    "print(\"Number of unique boards:\", len(boards_seen))\n",
    "print(\"Number of boards with at least one piece:\", boards_with_at_least_one_piece_count)\n",
    "print(\"Number of boards with at least two pieces:\", boards_with_at_least_two_pieces_count)\n",
    "print(\"Number of boards with at least three pieces:\", boards_with_at_least_three_pieces_count)\n",
    "print(\"Number of boards with at least four pieces:\", boards_with_at_least_four_pieces_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "def load_model(path):\n",
    "    model = NeuralNet(NETWORK_CONFIG)\n",
    "    model.load_state_dict(torch.load(path, weights_only=True))\n",
    "    model.to(DEVICE)\n",
    "    return model\n",
    "\n",
    "# base_model_1 = load_model(\"/Users/shivamsarodia/Dev/blockus/data/2024-12-02_21-22-57-notebook-models/base_in_notebook_1.pt\")\n",
    "# base_model_2 = load_model(\"/Users/shivamsarodia/Dev/blockus/data/2024-12-02_21-22-57-notebook-models/base_in_notebook_2.pt\")\n",
    "# ratio_two_model_1 = load_model(\"/Users/shivamsarodia/Dev/blockus/data/2024-12-02_21-22-57-notebook-models/sample_ratio_two_1.pt\")\n",
    "ratio_two_model_2 = load_model(\"/Users/shivamsarodia/Dev/blockus/data/2024-12-02_21-22-57-notebook-models/sample_ratio_two_2.pt\")\n",
    "\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2728/2728 [00:27<00:00, 100.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_loss': 0.019116447764486764,\n",
       " 'value_loss': 0.015213298658504423,\n",
       " 'policy_loss': 0.02470347422051892,\n",
       " 'value_max_correct': 0.571880298794739,\n",
       " 'policy_max_correct': 0.7852412813344943}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_test_losses(ratio_two_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 276/276 [00:05<00:00, 47.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to tensors...\n"
     ]
    }
   ],
   "source": [
    "test_game_files = glob.glob(\"/Users/shivamsarodia/Dev/blockus/data/2024-11-23_00-37-50-doublehandedness/more_games_we_didnt_train_on/*.npz\")\n",
    "test_gamedata = load_games_new(test_game_files, with_tqdm=True)\n",
    "\n",
    "print(\"Converting to tensors...\")\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "boards_tensor = torch.from_numpy(test_gamedata[\"boards\"]).to(dtype=torch.float, device=\"mps\")\n",
    "policies_tensor = torch.from_numpy(test_gamedata[\"policies\"]).to(dtype=torch.float, device=\"mps\")\n",
    "values_tensor = torch.from_numpy(test_gamedata[\"values\"]).to(dtype=torch.float, device=\"mps\")\n",
    "valid_moves = torch.from_numpy(test_gamedata[\"valid_moves\"]).to(dtype=torch.bool, device=\"mps\")\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(boards_tensor, policies_tensor, values_tensor, valid_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5261/5261 [00:52<00:00, 100.32it/s]\n",
      "100%|██████████| 5261/5261 [00:52<00:00, 100.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# base_model_1_test_result = get_test_losses(base_model_1)\n",
    "base_model_2_test_result = get_test_losses(base_model_2)\n",
    "# ratio_two_model_1_test_result = get_test_losses(ratio_two_model_1)\n",
    "ratio_two_model_2_test_result = get_test_losses(ratio_two_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241365"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_gamedata[\"boards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1078038"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "241365 + 336673 + 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_total': 0.019015346798564505,\n",
       " 'loss_value': 0.015089419780530288,\n",
       " 'loss_policy': 0.024847638171400108,\n",
       " 'value_max_correct': 0.5920462882381421,\n",
       " 'policy_max_correct': 0.7863327323545398}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_2_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_total': 0.019214975594640738,\n",
       " 'loss_value': 0.015309898972442953,\n",
       " 'loss_policy': 0.02471567374811644,\n",
       " 'value_max_correct': 0.5672061614682496,\n",
       " 'policy_max_correct': 0.7846664270672136}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_two_model_2_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_of_boolean_var(p):\n",
    "    standard_error = np.sqrt(p * (1 - p) / len(dataset))\n",
    "    return p + 2 * standard_error, p - 2 * standard_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "models = ['Base 1', 'Base 2', 'Ratio Two 1', 'Ratio Two 2']\n",
    "\n",
    "# Value accuracy plot\n",
    "value_accuracies = [\n",
    "    base_model_1_test_result[\"value_max_correct\"],\n",
    "    base_model_2_test_result[\"value_max_correct\"], \n",
    "    ratio_two_model_1_test_result[\"value_max_correct\"],\n",
    "    ratio_two_model_2_test_result[\"value_max_correct\"]\n",
    "]\n",
    "value_errors = [range_of_boolean_var(acc) for acc in value_accuracies]\n",
    "value_upper = [err[0] - acc for acc, err in zip(value_accuracies, value_errors)]\n",
    "value_lower = [acc - err[1] for acc, err in zip(value_accuracies, value_errors)]\n",
    "\n",
    "ax1.bar(models, value_accuracies, yerr=[value_lower, value_upper], capsize=5)\n",
    "ax1.set_title('Value Prediction Accuracy')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(bottom=min(value_accuracies) * 0.95)  # Set bottom to 95% of minimum value\n",
    "\n",
    "# Policy accuracy plot  \n",
    "policy_accuracies = [\n",
    "    base_model_1_test_result[\"policy_max_correct\"],\n",
    "    base_model_2_test_result[\"policy_max_correct\"],\n",
    "    ratio_two_model_1_test_result[\"policy_max_correct\"], \n",
    "    ratio_two_model_2_test_result[\"policy_max_correct\"]\n",
    "]\n",
    "policy_errors = [range_of_boolean_var(acc) for acc in policy_accuracies]\n",
    "policy_upper = [err[0] - acc for acc, err in zip(policy_accuracies, policy_errors)]\n",
    "policy_lower = [acc - err[1] for acc, err in zip(policy_accuracies, policy_errors)]\n",
    "\n",
    "ax2.bar(models, policy_accuracies, yerr=[policy_lower, policy_upper], capsize=5)\n",
    "ax2.set_title('Policy Prediction Accuracy')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_ylim(bottom=min(policy_accuracies) * 0.95)  # Set bottom to 95% of minimum value\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Base model 1\")\n",
    "print(base_model_1_test_result)\n",
    "print(\"Base model 2\")\n",
    "print(base_model_2_test_result)\n",
    "print(\"Ratio two model 1\")\n",
    "print(ratio_two_model_1_test_result)\n",
    "print(\"Ratio two model 2\")\n",
    "print(ratio_two_model_2_test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train files: 1836\n",
      "Num test files: 205\n",
      "Loading train games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1836/1836 [00:38<00:00, 48.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:04<00:00, 47.32it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m test_boards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m board \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[0;32m---> 33\u001b[0m     test_boards\u001b[38;5;241m.\u001b[39madd(\u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mtobytes())\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of unique test boards:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_boards))\n\u001b[1;32m     37\u001b[0m train_boards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "file_paths = glob.glob(\"/Users/shivamsarodia/Dev/blockus/data/2024-11-23_00-37-50-doublehandedness/untrained_games_*/*.npz\")\n",
    "\n",
    "random.seed(20554)\n",
    "random.shuffle(file_paths)\n",
    "\n",
    "num_train_games = int(len(file_paths) * 0.9)\n",
    "train_file_paths = file_paths[:num_train_games]\n",
    "test_file_paths = file_paths[num_train_games:]\n",
    "\n",
    "print(\"Num train files:\", len(train_file_paths))\n",
    "print(\"Num test files:\", len(test_file_paths))\n",
    "\n",
    "print(\"Loading train games...\")\n",
    "train_dataset = get_dataset(train_file_paths)\n",
    "\n",
    "print(\"Loading test games...\")\n",
    "test_dataset = get_dataset(test_file_paths)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total test boards: 181003\n",
      "Number of unique test boards: 101767\n",
      "Number of total train boards: 1612873\n",
      "Number of unique train boards: 781565\n",
      "Number of boards shared: 20435\n",
      "Number of rows of test data that appear in train data: 99330\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "test_boards = Counter()\n",
    "for board, _, _, _ in test_dataloader:\n",
    "    test_boards[board.numpy(force=True).tobytes()] += 1\n",
    "\n",
    "print(\"Number of total test boards:\", len(test_dataset))\n",
    "print(\"Number of unique test boards:\", len(test_boards))\n",
    "\n",
    "train_boards = Counter()\n",
    "for board, _, _, _ in train_dataloader:\n",
    "    train_boards[board.numpy(force=True).tobytes()] += 1\n",
    "\n",
    "print(\"Number of total train boards:\", len(train_dataset))\n",
    "print(\"Number of unique train boards:\", len(train_boards))\n",
    "\n",
    "test_boards_in_train = Counter()\n",
    "for board in test_boards:\n",
    "    if board in train_boards:\n",
    "        test_boards_in_train[board] = test_boards[board]\n",
    "\n",
    "print(\"Number of boards shared:\", len(test_boards_in_train))\n",
    "print(\"Number of rows of test data that appear in train data:\", sum(test_boards_in_train.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.548775434661304"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "99330 / 181003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.548775434661304"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "99330 / 181003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total test boards: 181003\n",
      "Number of total train boards: 1612873\n",
      "Number of boards shared: 20435\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total test boards:\", len(test_dataset))\n",
    "print(\"Number of total train boards:\", len(train_dataset))\n",
    "print(\"Number of boards shared:\", len(test_boards & train_boards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c1 = Counter()\n",
    "c1[4] += 1\n",
    "c2 = Counter()\n",
    "c2[5] += 3\n",
    "c1 & c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(c2.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5622392999011067"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "101767 / 181003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "for board, _, _, _ in train_dataset:\n",
    "    print(board.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
