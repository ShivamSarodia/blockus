{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivamsarodia/.pyenv/versions/3.11.3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-26 22:46:14,722\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config:  {\"development\": {\"debug_mode\": true, \"profile\": false, \"runtime\": 72000, \"display_logs_in_console\": false, \"output_directory\": \"data/2024-11-24_00-18-45-literatist\"}, \"logging\": {\"save_interval\": 3600, \"mcts_report_fraction\": 0, \"ucb_report\": false, \"gpu_evaluation\": true, \"made_move\": true}, \"game\": {\"board_size\": 10, \"num_moves\": 6233, \"moves_directory\": \"../data/moves_10\"}, \"architecture\": {\"gameplay_processes\": 6, \"coroutines_per_process\": 256, \"game_flush_threshold\": 200}, \"training\": {\"run\": true, \"network_name\": \"default\", \"batch_size\": 64, \"policy_loss_weight\": 0.158, \"learning_rate\": 0.001, \"sample_window\": 50000, \"samples_per_generation\": 10000, \"sampling_ratio\": 1.0, \"minimum_window_size\": 10000, \"new_data_check_interval\": 30}, \"networks\": {\"default\": {\"main_body_channels\": 64, \"value_head_channels\": 16, \"value_head_flat_layer_width\": 64, \"policy_head_channels\": 64, \"residual_blocks\": 8, \"model_path\": \"\", \"model_directory\": \"data/2024-11-24_00-18-45-literatist/models/\", \"new_model_check_interval\": 120, \"batch_size\": 128}}, \"agents\": [{\"type\": \"mcts\", \"network\": \"default\", \"full_move_probability\": 0.2, \"full_move_rollouts\": 500, \"fast_move_rollouts\": 100, \"ucb_exploration\": 1.4, \"total_dirichlet_alpha\": 10.83, \"root_exploration_fraction\": 0.25, \"name\": \"default\"}, {\"type\": \"mcts\", \"network\": \"default\", \"full_move_probability\": 0.2, \"full_move_rollouts\": 500, \"fast_move_rollouts\": 100, \"ucb_exploration\": 1.4, \"total_dirichlet_alpha\": 10.83, \"root_exploration_fraction\": 0.25, \"name\": \"default\"}, {\"type\": \"mcts\", \"network\": \"default\", \"full_move_probability\": 0.2, \"full_move_rollouts\": 500, \"fast_move_rollouts\": 100, \"ucb_exploration\": 1.4, \"total_dirichlet_alpha\": 10.83, \"root_exploration_fraction\": 0.25, \"name\": \"default\"}, {\"type\": \"mcts\", \"network\": \"default\", \"full_move_probability\": 0.2, \"full_move_rollouts\": 500, \"fast_move_rollouts\": 100, \"ucb_exploration\": 1.4, \"total_dirichlet_alpha\": 10.83, \"root_exploration_fraction\": 0.25, \"name\": \"default\"}]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import aim\n",
    "\n",
    "os.environ[\"CONFIG_PATHS\"] = \"../configs/self_play.yaml\"\n",
    "os.environ[\"CONFIG_OVERRIDES\"] = 'game.moves_directory=\"../data/moves_10\"'\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from display import Display\n",
    "from configuration import moves_data, config\n",
    "from training.actor import TrainingActor\n",
    "import training.helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_net import NeuralNet\n",
    "from training.game_data_manager import GameDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_CONFIG = config()[\"networks\"][\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: piece_indices\n",
      "Loading file: rotation_mapping\n",
      "Loading file: new_occupieds\n",
      "Loading file: moves_ruled_out_for_all\n",
      "Loading file: scores\n",
      "Loading file: moves_ruled_out_for_player\n",
      "Loading file: moves_enabled_for_player\n",
      "Loading file: new_adjacents\n",
      "Loading file: new_corners\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "DEVICE = \"mps\"\n",
    "MOVES = moves_data()\n",
    "GAMES_DIR = \"../data/2024-11-24_00-18-45-literatist/games\"\n",
    "WINDOW_SIZE = 50000\n",
    "MINIMUM_WINDOW_SIZE = 10000\n",
    "POLICY_LOSS_WEIGHT = 0.158\n",
    "LEARNING_RATE = 1e-3\n",
    "SAMPLING_RATIO = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data_manager = GameDataManager(GAMES_DIR, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(NETWORK_CONFIG)\n",
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size:  50000\n",
      "Cumulative window fed:  642275\n"
     ]
    }
   ],
   "source": [
    "training.helpers.feed_window_until_amount(\n",
    "    game_data_manager,\n",
    "    642275,\n",
    "    1e6,\n",
    ")\n",
    "print(\"Window size: \", game_data_manager.current_window_size())\n",
    "print(\"Cumulative window fed: \", game_data_manager.cumulative_window_fed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, window size 10000, cumulative window fed 10000\n",
      "Batch 100, window size 16400, cumulative window fed 16400\n",
      "Batch 200, window size 22800, cumulative window fed 22800\n",
      "Batch 300, window size 29200, cumulative window fed 29200\n",
      "Batch 400, window size 35600, cumulative window fed 35600\n",
      "Batch 500, window size 42000, cumulative window fed 42000\n",
      "Batch 600, window size 48400, cumulative window fed 48400\n",
      "Batch 700, window size 50000, cumulative window fed 54800\n",
      "Batch 800, window size 50000, cumulative window fed 61200\n",
      "Batch 900, window size 50000, cumulative window fed 67600\n",
      "Batch 1000, window size 50000, cumulative window fed 74000\n",
      "Batch 1100, window size 50000, cumulative window fed 80400\n",
      "Batch 1200, window size 50000, cumulative window fed 86800\n",
      "Batch 1300, window size 50000, cumulative window fed 93200\n",
      "Batch 1400, window size 50000, cumulative window fed 99600\n",
      "Batch 1500, window size 50000, cumulative window fed 106000\n",
      "Batch 1600, window size 50000, cumulative window fed 112400\n",
      "Batch 1700, window size 50000, cumulative window fed 118800\n",
      "Batch 1800, window size 50000, cumulative window fed 125200\n",
      "Batch 1900, window size 50000, cumulative window fed 131600\n",
      "Batch 2000, window size 50000, cumulative window fed 138000\n",
      "Batch 2100, window size 50000, cumulative window fed 144400\n",
      "Batch 2200, window size 50000, cumulative window fed 150800\n",
      "Batch 2300, window size 50000, cumulative window fed 157200\n",
      "Batch 2400, window size 50000, cumulative window fed 163600\n",
      "Batch 2500, window size 50000, cumulative window fed 170000\n",
      "Batch 2600, window size 50000, cumulative window fed 176400\n",
      "Batch 2700, window size 50000, cumulative window fed 182800\n",
      "Batch 2800, window size 50000, cumulative window fed 189200\n",
      "Batch 2900, window size 50000, cumulative window fed 195600\n",
      "Batch 3000, window size 50000, cumulative window fed 202000\n",
      "Batch 3100, window size 50000, cumulative window fed 208400\n",
      "Batch 3200, window size 50000, cumulative window fed 214800\n",
      "Batch 3300, window size 50000, cumulative window fed 221200\n",
      "Batch 3400, window size 50000, cumulative window fed 227600\n",
      "Batch 3500, window size 50000, cumulative window fed 234000\n",
      "Batch 3600, window size 50000, cumulative window fed 240400\n",
      "Batch 3700, window size 50000, cumulative window fed 246800\n",
      "Batch 3800, window size 50000, cumulative window fed 253200\n",
      "Batch 3900, window size 50000, cumulative window fed 259600\n",
      "Batch 4000, window size 50000, cumulative window fed 266000\n",
      "Batch 4100, window size 50000, cumulative window fed 272400\n",
      "Batch 4200, window size 50000, cumulative window fed 278800\n",
      "Batch 4300, window size 50000, cumulative window fed 285200\n",
      "Batch 4400, window size 50000, cumulative window fed 291600\n",
      "Batch 4500, window size 50000, cumulative window fed 298000\n",
      "Batch 4600, window size 50000, cumulative window fed 304400\n",
      "Batch 4700, window size 50000, cumulative window fed 310800\n",
      "Batch 4800, window size 50000, cumulative window fed 317200\n",
      "Batch 4900, window size 50000, cumulative window fed 323600\n",
      "Batch 5000, window size 50000, cumulative window fed 330000\n",
      "Batch 5100, window size 50000, cumulative window fed 336400\n",
      "Batch 5200, window size 50000, cumulative window fed 342800\n",
      "Batch 5300, window size 50000, cumulative window fed 349200\n",
      "Batch 5400, window size 50000, cumulative window fed 355600\n",
      "Batch 5500, window size 50000, cumulative window fed 362000\n",
      "Batch 5600, window size 50000, cumulative window fed 368400\n",
      "Batch 5700, window size 50000, cumulative window fed 374800\n",
      "Batch 5800, window size 50000, cumulative window fed 381200\n",
      "Batch 5900, window size 50000, cumulative window fed 387600\n",
      "Batch 6000, window size 50000, cumulative window fed 394000\n",
      "Batch 6100, window size 50000, cumulative window fed 400400\n",
      "Batch 6200, window size 50000, cumulative window fed 406800\n",
      "Batch 6300, window size 50000, cumulative window fed 413200\n",
      "Batch 6400, window size 50000, cumulative window fed 419600\n",
      "Batch 6500, window size 50000, cumulative window fed 426000\n",
      "Batch 6600, window size 50000, cumulative window fed 432400\n",
      "Batch 6700, window size 50000, cumulative window fed 438800\n",
      "Batch 6800, window size 50000, cumulative window fed 445200\n",
      "Batch 6900, window size 50000, cumulative window fed 451600\n",
      "Batch 7000, window size 50000, cumulative window fed 458000\n",
      "Batch 7100, window size 50000, cumulative window fed 464400\n",
      "Batch 7200, window size 50000, cumulative window fed 470800\n",
      "Batch 7300, window size 50000, cumulative window fed 477200\n",
      "Batch 7400, window size 50000, cumulative window fed 483600\n",
      "Batch 7500, window size 50000, cumulative window fed 490000\n",
      "Batch 7600, window size 50000, cumulative window fed 496400\n",
      "Batch 7700, window size 50000, cumulative window fed 502800\n",
      "Batch 7800, window size 50000, cumulative window fed 509200\n",
      "Batch 7900, window size 50000, cumulative window fed 515600\n",
      "Batch 8000, window size 50000, cumulative window fed 522000\n",
      "Batch 8100, window size 50000, cumulative window fed 528400\n",
      "Batch 8200, window size 50000, cumulative window fed 534800\n",
      "Batch 8300, window size 50000, cumulative window fed 541200\n",
      "Batch 8400, window size 50000, cumulative window fed 547600\n",
      "Batch 8500, window size 50000, cumulative window fed 554000\n",
      "Batch 8600, window size 50000, cumulative window fed 560400\n",
      "Batch 8700, window size 50000, cumulative window fed 566800\n",
      "Batch 8800, window size 50000, cumulative window fed 573200\n",
      "Batch 8900, window size 50000, cumulative window fed 579600\n",
      "Batch 9000, window size 50000, cumulative window fed 586000\n",
      "Batch 9100, window size 50000, cumulative window fed 592400\n",
      "Batch 9200, window size 50000, cumulative window fed 598800\n",
      "Batch 9300, window size 50000, cumulative window fed 605200\n",
      "Batch 9400, window size 50000, cumulative window fed 611600\n",
      "Batch 9500, window size 50000, cumulative window fed 618000\n",
      "Batch 9600, window size 50000, cumulative window fed 624400\n",
      "Batch 9700, window size 50000, cumulative window fed 630800\n",
      "Batch 9800, window size 50000, cumulative window fed 637200\n",
      "Batch 9900, window size 50000, cumulative window fed 643600\n",
      "Batch 10000, window size 50000, cumulative window fed 650000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, window size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgame_data_manager\u001b[38;5;241m.\u001b[39mcurrent_window_size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, cumulative window fed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgame_data_manager\u001b[38;5;241m.\u001b[39mcumulative_window_fed()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m training_result \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhelpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop_iteration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgame_data_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAMPLING_RATIO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPOLICY_LOSS_WEIGHT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m training_result:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Dev/blockus/analysis/../src/training/helpers.py:37\u001b[0m, in \u001b[0;36mloop_iteration\u001b[0;34m(model, optimizer, game_data_manager, device, batch_size, sampling_ratio, policy_loss_weight)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloop_iteration\u001b[39m(\n\u001b[1;32m     23\u001b[0m     model,\n\u001b[1;32m     24\u001b[0m     optimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# we can train on the batch we just read. The floor/random bits ensure we round\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# correctly, e.g. 2.1 rounds to 2 90% of the time and 3 10% of the time.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     required_new_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mfloor(batch_size \u001b[38;5;241m/\u001b[39m sampling_ratio \u001b[38;5;241m+\u001b[39m random\u001b[38;5;241m.\u001b[39mrandom()))\n\u001b[0;32m---> 37\u001b[0m     ingestion_count \u001b[38;5;241m=\u001b[39m \u001b[43mgame_data_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequired_new_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire_exact_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Not enough new data to train a batch. Try again later.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ingestion_count:\n",
      "File \u001b[0;32m~/Dev/blockus/analysis/../src/training/game_data_manager.py:74\u001b[0m, in \u001b[0;36mGameDataManager.feed_window\u001b[0;34m(self, max_samples_to_feed, require_exact_count)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03mRead up to max_samples_to_feed new samples into the current sample window.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03mDrop any old samples that have fallen out of the window.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mReturn the number of samples read.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Load new samples from disk if possible.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# E.g. suppose we have 100 samples in gamedata, and the window size is currently 95.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# If we're asked to feed the window 20 samples, we'll attempt to load at least 15 more\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# it would be prohibitively expensive in the offline training context where we have many\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# samples on disk that we cannot load into memory at once.\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_unread_gamedata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_samples_to_feed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamedata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_window_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Check how many samples we have beyond the window to see if we can feed into\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# the window the desired amount.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m num_samples_after_window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamedata) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_window_size\n",
      "File \u001b[0;32m~/Dev/blockus/analysis/../src/training/game_data_manager.py:51\u001b[0m, in \u001b[0;36mGameDataManager._load_unread_gamedata\u001b[0;34m(self, samples_requested)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_gamedata_paths\u001b[38;5;241m.\u001b[39madd(path)\n\u001b[1;32m     49\u001b[0m         total_samples_loaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 51\u001b[0m game_data_tuple \u001b[38;5;241m=\u001b[39m \u001b[43mload_games\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths_to_load\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(game_data_tuple[\u001b[38;5;241m0\u001b[39m])):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamedata\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m     55\u001b[0m             game_data_category[i]\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m game_data_category \u001b[38;5;129;01min\u001b[39;00m game_data_tuple\n\u001b[1;32m     57\u001b[0m         )\n\u001b[1;32m     58\u001b[0m     )\n",
      "File \u001b[0;32m~/Dev/blockus/analysis/../src/training/load_games.py:26\u001b[0m, in \u001b[0;36mload_games\u001b[0;34m(game_file_paths)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m BadZipFile:\n\u001b[1;32m     23\u001b[0m             log_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbad_game_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: game_file})\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moccupancies\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     27\u001b[0m     np\u001b[38;5;241m.\u001b[39mconcatenate(policies),\n\u001b[1;32m     28\u001b[0m     np\u001b[38;5;241m.\u001b[39mconcatenate(final_game_values),\n\u001b[1;32m     29\u001b[0m     np\u001b[38;5;241m.\u001b[39mconcatenate(average_rollout_values),\n\u001b[1;32m     30\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "run = aim.Run(repo='/tmp/.aim')\n",
    "\n",
    "batch_index = 0\n",
    "while True:\n",
    "    if batch_index % 100 == 0:\n",
    "        print(f\"Batch {batch_index}, window size {game_data_manager.current_window_size()}, cumulative window fed {game_data_manager.cumulative_window_fed()}\")\n",
    "\n",
    "    training_result = training.helpers.loop_iteration(\n",
    "        model,\n",
    "        optimizer,\n",
    "        game_data_manager,\n",
    "        device=DEVICE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampling_ratio=SAMPLING_RATIO,\n",
    "        policy_loss_weight=POLICY_LOSS_WEIGHT,\n",
    "    )\n",
    "    if not training_result:\n",
    "        break\n",
    "\n",
    "    for key, value in training_result.items():\n",
    "        run.track(\n",
    "            value,\n",
    "            name=key,\n",
    "            step=batch_index,\n",
    "        )\n",
    "    batch_index += 1\n",
    "\n",
    "run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../data/2024-11-26_19-59-19-prodigal/models/trained_in_notebook.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_paths = [\n",
    "#     os.path.join(GAMES_DIR, f)\n",
    "#     for f in os.listdir(GAMES_DIR)\n",
    "# ]\n",
    "\n",
    "# # Train on first 3/4, test on last 1/4.\n",
    "# train_path_index_cutoff = 3 * len(game_paths) // 4\n",
    "\n",
    "# train_paths = sorted(game_paths)[:train_path_index_cutoff]\n",
    "# test_paths = sorted(game_paths)[train_path_index_cutoff:]\n",
    "\n",
    "# def paths_to_dataloader(paths):\n",
    "#     boards, policies, values = load_old_format_games(paths)\n",
    "#     dataset = TensorDataset(\n",
    "#         torch.Tensor(boards),\n",
    "#         torch.Tensor(policies),\n",
    "#         torch.Tensor(values),\n",
    "#     )\n",
    "#     return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# train_dataloader = paths_to_dataloader(train_paths)\n",
    "# test_dataloader = paths_to_dataloader(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = aim.Run(repo='/tmp/.aim')\n",
    "\n",
    "# model = NeuralNet(config()[\"networks\"][\"default\"]).to(DEVICE)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# for batch_index, (boards, policies, values) in tqdm(enumerate(train_dataloader)):\n",
    "#     boards = boards.to(DEVICE)\n",
    "#     policies = policies.to(DEVICE)\n",
    "#     values = values.to(DEVICE)\n",
    "\n",
    "#     pred_values, pred_policy = model(boards)\n",
    "#     value_loss = nn.CrossEntropyLoss()(\n",
    "#         pred_values,\n",
    "#         values,\n",
    "#     )\n",
    "#     policy_loss = 0.158 * nn.CrossEntropyLoss()(\n",
    "#         pred_policy,\n",
    "#         policies,\n",
    "#     )\n",
    "#     loss = value_loss + policy_loss\n",
    "\n",
    "#     run.track(\n",
    "#         value_loss.item(),\n",
    "#         name=\"value_loss\",\n",
    "#         step=batch_index,\n",
    "#     )\n",
    "#     run.track(\n",
    "#         policy_loss.item(),\n",
    "#         name=\"policy_loss\",\n",
    "#         step=batch_index,\n",
    "#     )\n",
    "#     run.track(\n",
    "#         loss.item(),\n",
    "#         name=\"total_loss\",\n",
    "#         step=batch_index,\n",
    "#     )\n",
    "\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "# run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_losses = []\n",
    "# policy_losses = []\n",
    "# total_losses = []\n",
    "\n",
    "# for batch_index, (boards, policies, values) in tqdm(enumerate(test_dataloader)):\n",
    "#     pred_values, pred_policy = model(boards)\n",
    "#     value_loss = nn.CrossEntropyLoss()(\n",
    "#         pred_values,\n",
    "#         values,\n",
    "#     )\n",
    "#     policy_loss = 0.158 * nn.CrossEntropyLoss()(\n",
    "#         pred_policy,\n",
    "#         policies,\n",
    "#     )\n",
    "#     loss = value_loss + policy_loss\n",
    "\n",
    "#     value_losses.append(value_loss.item())\n",
    "#     policy_losses.append(policy_loss.item())\n",
    "#     total_losses.append(loss.item())\n",
    "\n",
    "# print(\"Average value loss: \", sum(value_losses) / len(value_losses))\n",
    "# print(\"Average policy loss: \", sum(policy_losses) / len(policy_losses))\n",
    "# print(\"Average total loss: \", sum(total_losses) / len(total_losses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
