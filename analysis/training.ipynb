{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import aim\n",
    "\n",
    "os.environ[\"CONFIG_PATHS\"] = \"../configs/self_play.yaml\"\n",
    "os.environ[\"CONFIG_OVERRIDES\"] = 'game.moves_directory=\"../data/moves_10\"'\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from display import Display\n",
    "from configuration import moves_data, config\n",
    "from training.actor import TrainingActor\n",
    "import training.helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_net import NeuralNet\n",
    "from training.game_data_manager import GameDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_CONFIG = config()[\"networks\"][\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "DEVICE = \"mps\"\n",
    "MOVES = moves_data()\n",
    "GAMES_DIR = \"../data/2024-11-23_00-37-50-doublehandedness/games\"\n",
    "WINDOW_SIZE = 50000\n",
    "MINIMUM_WINDOW_SIZE = 10000\n",
    "POLICY_LOSS_WEIGHT = 0.158\n",
    "LEARNING_RATE = 1e-3\n",
    "SAMPLING_RATIO = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data_manager = GameDataManager(GAMES_DIR, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(NETWORK_CONFIG)\n",
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size:  10000\n",
      "Cumulative window fed:  10000\n"
     ]
    }
   ],
   "source": [
    "training.helpers.feed_window_until_amount(\n",
    "    game_data_manager,\n",
    "    MINIMUM_WINDOW_SIZE,\n",
    "    1e6,\n",
    ")\n",
    "print(\"Window size: \", game_data_manager.current_window_size())\n",
    "print(\"Cumulative window fed: \", game_data_manager.cumulative_window_fed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, window size 0, cumulative window fed 0\n",
      "Batch 100, window size 6400, cumulative window fed 6400\n",
      "Batch 200, window size 12800, cumulative window fed 12800\n",
      "Batch 300, window size 19200, cumulative window fed 19200\n",
      "Batch 400, window size 25600, cumulative window fed 25600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, window size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgame_data_manager\u001b[38;5;241m.\u001b[39mcurrent_window_size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, cumulative window fed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgame_data_manager\u001b[38;5;241m.\u001b[39mcumulative_window_fed()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m training_result \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhelpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop_iteration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgame_data_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAMPLING_RATIO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPOLICY_LOSS_WEIGHT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m training_result:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Dev/blockus/analysis/../src/training/helpers.py:66\u001b[0m, in \u001b[0;36mloop_iteration\u001b[0;34m(model, optimizer, game_data_manager, device, batch_size, sampling_ratio, policy_loss_weight)\u001b[0m\n\u001b[1;32m     62\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mvalue_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: policy_loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(boards),\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcumulative_window_fed\u001b[39m\u001b[38;5;124m\"\u001b[39m: game_data_manager\u001b[38;5;241m.\u001b[39mcumulative_window_fed(),\n\u001b[1;32m     71\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run = aim.Run(repo='/tmp/.aim')\n",
    "\n",
    "batch_index = 0\n",
    "while True:\n",
    "    if batch_index % 100 == 0:\n",
    "        print(f\"Batch {batch_index}, window size {game_data_manager.current_window_size()}, cumulative window fed {game_data_manager.cumulative_window_fed()}\")\n",
    "\n",
    "    training_result = training.helpers.loop_iteration(\n",
    "        model,\n",
    "        optimizer,\n",
    "        game_data_manager,\n",
    "        device=DEVICE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampling_ratio=SAMPLING_RATIO,\n",
    "        policy_loss_weight=POLICY_LOSS_WEIGHT,\n",
    "    )\n",
    "    if not training_result:\n",
    "        break\n",
    "\n",
    "    for key, value in training_result.items():\n",
    "        run.track(\n",
    "            value,\n",
    "            name=key,\n",
    "            step=batch_index,\n",
    "        )\n",
    "    batch_index += 1\n",
    "\n",
    "run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../data/2024-11-26_19-59-19-prodigal/models/trained_in_notebook.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_paths = [\n",
    "#     os.path.join(GAMES_DIR, f)\n",
    "#     for f in os.listdir(GAMES_DIR)\n",
    "# ]\n",
    "\n",
    "# # Train on first 3/4, test on last 1/4.\n",
    "# train_path_index_cutoff = 3 * len(game_paths) // 4\n",
    "\n",
    "# train_paths = sorted(game_paths)[:train_path_index_cutoff]\n",
    "# test_paths = sorted(game_paths)[train_path_index_cutoff:]\n",
    "\n",
    "# def paths_to_dataloader(paths):\n",
    "#     boards, policies, values = load_old_format_games(paths)\n",
    "#     dataset = TensorDataset(\n",
    "#         torch.Tensor(boards),\n",
    "#         torch.Tensor(policies),\n",
    "#         torch.Tensor(values),\n",
    "#     )\n",
    "#     return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# train_dataloader = paths_to_dataloader(train_paths)\n",
    "# test_dataloader = paths_to_dataloader(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = aim.Run(repo='/tmp/.aim')\n",
    "\n",
    "# model = NeuralNet(config()[\"networks\"][\"default\"]).to(DEVICE)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# for batch_index, (boards, policies, values) in tqdm(enumerate(train_dataloader)):\n",
    "#     boards = boards.to(DEVICE)\n",
    "#     policies = policies.to(DEVICE)\n",
    "#     values = values.to(DEVICE)\n",
    "\n",
    "#     pred_values, pred_policy = model(boards)\n",
    "#     value_loss = nn.CrossEntropyLoss()(\n",
    "#         pred_values,\n",
    "#         values,\n",
    "#     )\n",
    "#     policy_loss = 0.158 * nn.CrossEntropyLoss()(\n",
    "#         pred_policy,\n",
    "#         policies,\n",
    "#     )\n",
    "#     loss = value_loss + policy_loss\n",
    "\n",
    "#     run.track(\n",
    "#         value_loss.item(),\n",
    "#         name=\"value_loss\",\n",
    "#         step=batch_index,\n",
    "#     )\n",
    "#     run.track(\n",
    "#         policy_loss.item(),\n",
    "#         name=\"policy_loss\",\n",
    "#         step=batch_index,\n",
    "#     )\n",
    "#     run.track(\n",
    "#         loss.item(),\n",
    "#         name=\"total_loss\",\n",
    "#         step=batch_index,\n",
    "#     )\n",
    "\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "# run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_losses = []\n",
    "# policy_losses = []\n",
    "# total_losses = []\n",
    "\n",
    "# for batch_index, (boards, policies, values) in tqdm(enumerate(test_dataloader)):\n",
    "#     pred_values, pred_policy = model(boards)\n",
    "#     value_loss = nn.CrossEntropyLoss()(\n",
    "#         pred_values,\n",
    "#         values,\n",
    "#     )\n",
    "#     policy_loss = 0.158 * nn.CrossEntropyLoss()(\n",
    "#         pred_policy,\n",
    "#         policies,\n",
    "#     )\n",
    "#     loss = value_loss + policy_loss\n",
    "\n",
    "#     value_losses.append(value_loss.item())\n",
    "#     policy_losses.append(policy_loss.item())\n",
    "#     total_losses.append(loss.item())\n",
    "\n",
    "# print(\"Average value loss: \", sum(value_losses) / len(value_losses))\n",
    "# print(\"Average policy loss: \", sum(policy_losses) / len(policy_losses))\n",
    "# print(\"Average total loss: \", sum(total_losses) / len(total_losses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
