development:
  debug_mode: true
  profile: false
  runtime: 0
  display_logs_in_console: false
  output_directory: "data/2024-11-24_00-18-45-literatist"

logging:
  save_interval: 3600
  mcts_report_fraction: 0
  ucb_report: false
  gpu_evaluation: true
  made_move: true

game:
  board_size: 10
  num_moves: 6233
  moves_directory: "data/moves_10/"

architecture:
  gameplay_processes: 6
  coroutines_per_process: 256

  # Number of games to accumulate before they're flushed to disk.
  game_flush_threshold: 200

default_network:
  main_body_channels: 64
  value_head_channels: 16
  value_head_flat_layer_width: 64
  policy_head_channels: 64
  residual_blocks: 8

  # One of model_path or model_directory should be specified.
  model_path: ""
  model_directory: "data/2024-11-24_00-18-45-literatist/models/"

  # Frequency to check if there's a new model to load from disk.
  new_model_check_interval: 120
  batch_size: 128

individual_networks:
  default: {}

training:
  run: true
  network_name: "default"

  batch_size: 64

  policy_loss_weight: 0.158
  learning_rate: 1.0e-3
  
  # Maximum number of samples into the past to consider for training.
  sample_window: 50000
  # Number of samples that each generation will be trained on.
  samples_per_generation: 10000
  # Approximately how often each sample will be used for training. 
  # (This controls how much data must be available before a new generation is trained.)
  sampling_ratio: 1.0
  # We feed the window until it reaches this size before starting any training.
  minimum_window_size: 10000
  new_data_check_interval: 60

default_agent:
  type: "mcts"
  network: "default"
  full_move_probability: 0.2
  full_move_rollouts: 500
  fast_move_rollouts: 100
  # Very haphazard guess at an exploration parameter.
  ucb_exploration: 1.4
  # This mimics the value in AlphaZero using the logic from KataGo.
  total_dirichlet_alpha: 10.83
  root_exploration_fraction: 0.25

individual_agents:
  - name: "default"
  - name: "default"
  - name: "default"
  - name: "default"